{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Bandits: An Application of Thompson Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements the Thompson Sampling algorithm described in [Analysis of Thompson Sampling for the Multi-Armed Bandit Problem](http://proceedings.mlr.press/v23/agrawal12/agrawal12.pdf) by Shipra Agrawal and Navin Goyal and applies it to the Bayesian Bandit problem.\n",
    "\n",
    "** Note that \"bandit\" and \"arm\" are used interchangeably in this notebook (and in the Online Learning literature in general). **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "[Section 1: Thompson Sampling for Bernoulli Bandits](#section1)\n",
    "\n",
    "[Section 2: Thompson Sampling for General Stochastic Bandits](#section2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Thompson Sampling for Bernoulli Bandits <a name=\"section1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first examine the Bernoulli Bandit case where:\n",
    "* Rewards $r_t \\in \\{0, 1\\}$\n",
    "* For arm $i$, the probability of success ($r_{t,i} = 1$) is its mean, $\\mu_i$\n",
    "\n",
    "We maintain a Bayesian prior on the arm means $\\mu_i$. For Bernoulli rewards (either 0 or 1), it turns out that a Beta distribution is a convenient choice of priors. This is because if the prior is a Beta($\\alpha$, $\\beta$) distribution, then updating the posterior distributions become much simpler. After observing a Bernoulli trial, the posterior distribution is:\n",
    "* Beta($\\alpha+1$, $\\beta$) if the trial succeeded (reward = 1)\n",
    "* Beta($\\alpha$, $\\beta+1$) if the trial failed (reward = 0)\n",
    "\n",
    "The Thompson Sampling algorithm initializes a uniform prior on all arms, meaning that arm $i$ has a prior Beta(1,1) on $\\mu_i$ because Beta(1,1) is a uniform distribution on (0,1).\n",
    "\n",
    "Define:\n",
    "* $S_i(t)$ = number of successes (reward = 1) for arm $i$ up to time $t$\n",
    "* $F_i(t)$ = number of failures (reward = 0) for arm $i$ up to time $t$\n",
    "\n",
    "The algorithm will then update the distribution on $\\mu_i$ as Beta($S_i(t)+1$, $F_i(t)+1$), sample from these posterior distributions, and play an arm according to the probability of its mean being the largest.\n",
    "\n",
    "---\n",
    "#### Algorithm 1: Thompson Sampling for Bernoulli Bandits\n",
    "For each arm $i=1,...,N$, set $S_i=0$, $F_i=0$\n",
    "\n",
    "For each  $t=1,2,...$ do\n",
    "\n",
    "* For each arm $i=1,...,N$, sample $\\theta_i(t)$ from the Beta($S_i+1$, $F_i+1$) distribution\n",
    "* Play arm $i(t) := argmax_i$ $\\theta_i(t)$ and observe reward $r_t$\n",
    "* If $r_t=1$, then $S_i(t) = S_i(t)+1$, else $F_i(t) = F_i(t)+1$\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliBandit(object):\n",
    "    def __init__(self, num_bandits):\n",
    "        \"\"\"num_bandits must be at least 2\"\"\"\n",
    "        \n",
    "        assert num_bandits >= 2, \"Number of bandits must be at least 2\"\n",
    "        \n",
    "        # Initialize the means for each of the bandits randomly\n",
    "        self.bandits = np.random.random_sample(num_bandits)\n",
    "        self.num_bandits = len(self.bandits)\n",
    "        # Keep track of the largest probability to use in regret calculation as \"best bandit\"\n",
    "        self.max_prob = np.max(self.bandits)\n",
    "    \n",
    "    def chooseBandit(self, bandit_num):\n",
    "        \"\"\"\n",
    "        Chooses the indicated bandit and returns True if we get a reward, False otherwise.\n",
    "        \"\"\"\n",
    "        # Index out of range\n",
    "        assert bandit_num >= 0 and bandit_num < self.num_bandits, \\\n",
    "            \"Index Out of Range: {}. There are {} bandits.\".format(bandit_num, self.num_bandits)\n",
    "            \n",
    "        # Sample from Bernoulli distribution with probability of success equal to the bandit mean\n",
    "        return np.random.binomial(1, self.bandits[bandit_num]) == 1\n",
    "    \n",
    "    def computeRegret(self, bandit_num):\n",
    "        \"\"\"\n",
    "        Computes the regret accrued by the indicated bandit. By definition, this is how much reward we lost by\n",
    "        choosing the indicated bandit instead of the best one (indicated by the bandit with the largest mean).\n",
    "        \"\"\"\n",
    "        # Index out of range\n",
    "        assert bandit_num >= 0 and bandit_num < self.num_bandits, \\\n",
    "            \"Index Out of Range: {}. There are {} bandits.\".format(bandit_num, self.num_bandits)\n",
    "        \n",
    "        # Difference in expected reward for choosing a suboptimal bandit\n",
    "        return self.max_prob - self.bandits[bandit_num]\n",
    "\n",
    "    \n",
    "def draw_bandit_distribution(stats):\n",
    "    \"\"\"\n",
    "    Samples the probability of each bandit yielding a reward given the number of successes and failures so far.\n",
    "    This is computed using the Beta distribution.\n",
    "    \n",
    "    stats = [{'num_wins': __, 'num_losses': __}, ...]\n",
    "    len(stats) should be equal to the number of bandits\n",
    "    \"\"\"\n",
    "    reward_probs = np.zeros(len(stats))\n",
    "    \n",
    "    for bandit_num in range(len(stats)):\n",
    "        num_wins, num_losses = stats[bandit_num]['num_wins'], stats[bandit_num]['num_losses']\n",
    "        # The alpha parameter is num_wins+1 and the beta parameter is num_losses+1\n",
    "        reward_probs[bandit_num] = np.random.beta(num_wins+1, num_losses+1)\n",
    "    \n",
    "    return reward_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Thompson Sampling for General Stochastic Bandits <a name=\"section2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
