{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Bandits: An Application of Thompson Sampling\n",
    "\n",
    "#### EE 290S/CS 194: Machine Learning for Sequential Decision Making Under Uncertainty (Fall 2018)\n",
    "\n",
    "#### Authors: Julian Chan, Daniel Ho, Bernie Wang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements the Thompson Sampling algorithm described in [Analysis of Thompson Sampling for the Multi-Armed Bandit Problem](http://proceedings.mlr.press/v23/agrawal12/agrawal12.pdf) by Shipra Agrawal and Navin Goyal and applies it to the Bayesian Bandit problem.\n",
    "\n",
    "** Note that \"bandit\" and \"arm\" are used interchangeably in this notebook (and in the Online Learning literature in general). **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "[Section 1: Thompson Sampling for Bernoulli Bandits](#section1)\n",
    "\n",
    "[Section 2: Thompson Sampling for General Stochastic Bandits](#section2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Thompson Sampling for Bernoulli Bandits <a name=\"section1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first examine the Bernoulli Bandit case where:\n",
    "* Rewards $r_t \\in \\{0, 1\\}$ drawn from a Bernoulli distribution\n",
    "* For arm $i$, the probability of success ($r_{t,i} = 1$) is its mean, $\\mu_i$\n",
    "\n",
    "We maintain a Bayesian prior on the arm means $\\mu_i$. For Bernoulli rewards (either 0 or 1), it turns out that a Beta distribution is a convenient choice of priors. This is because if the prior is a Beta($\\alpha$, $\\beta$) distribution, then updating the posterior distributions become much simpler. After observing a Bernoulli trial, the posterior distribution is:\n",
    "* Beta($\\alpha+1$, $\\beta$) if the trial succeeded (reward = 1)\n",
    "* Beta($\\alpha$, $\\beta+1$) if the trial failed (reward = 0)\n",
    "\n",
    "The Thompson Sampling algorithm initializes a uniform prior on all arms, meaning that arm $i$ has a prior Beta(1,1) on $\\mu_i$ because Beta(1,1) is a uniform distribution on (0,1).\n",
    "\n",
    "Define:\n",
    "* $S_i(t)$ = number of successes (reward = 1) for arm $i$ up to time $t$\n",
    "* $F_i(t)$ = number of failures (reward = 0) for arm $i$ up to time $t$\n",
    "\n",
    "The algorithm will then update the distribution on $\\mu_i$ as Beta($S_i(t)+1$, $F_i(t)+1$), sample from these posterior distributions, and play an arm according to the probability of its mean being the largest.\n",
    "\n",
    "---\n",
    "#### Algorithm 1: Thompson Sampling for Bernoulli Bandits\n",
    "For each arm $i=1,...,N$, set $S_i(1)=0$, $F_i(1)=0$\n",
    "\n",
    "For each  $t=1,2,...$ do\n",
    "\n",
    "* For each arm $i=1,...,N$, sample $\\theta_i(t)$ from the Beta($S_i(t)+1$, $F_i(t)+1$) distribution\n",
    "* Play arm $i(t) := argmax_i$ $\\theta_i(t)$ and observe reward $r_t$\n",
    "* If $r_t=1$, then $S_i(t) = S_i(t)+1$, else $F_i(t) = F_i(t)+1$\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: When constructing bandits below, create subclass of MultiArmedBandit to inherit class methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiArmedBandit(object):\n",
    "    def __init__(self, num_bandits):\n",
    "        \"\"\"num_bandits must be at least 2\"\"\"\n",
    "        \n",
    "        assert num_bandits >= 2, \"Number of bandits must be at least 2\"\n",
    "        \n",
    "        # Initialize the means for each of the bandits randomly\n",
    "        self.bandits = np.random.sample(num_bandits)\n",
    "        self.num_bandits = num_bandits\n",
    "        # Keep track of the largest probability to use in regret calculation as \"best bandit\"\n",
    "        self.max_prob = np.max(self.bandits)\n",
    "    \n",
    "    # Should define this method when extending the class\n",
    "    def chooseBandit(self, bandit_num):\n",
    "        \"\"\"\n",
    "        Chooses the indicated bandit and returns True if we get a reward, False otherwise.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def computeRegret(self, bandit_num):\n",
    "        \"\"\"\n",
    "        Computes the regret accrued by the indicated bandit. By definition, this is how much reward we lost by\n",
    "        choosing the indicated bandit instead of the best one (indicated by the bandit with the largest mean).\n",
    "        \"\"\"\n",
    "        # Index out of range\n",
    "        assert bandit_num >= 0 and bandit_num < self.num_bandits, \\\n",
    "            \"Index Out of Range: {}. There are {} bandits.\".format(bandit_num, self.num_bandits)\n",
    "        \n",
    "        # Difference in expected reward for choosing a suboptimal bandit\n",
    "        return self.max_prob - self.bandits[bandit_num]\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Print out the means for each bandit arm.\"\"\"\n",
    "        return \"Bandits: \" + self.bandits.__str__()\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__repr__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement chooseBandit function. Returns True if bandit yields reward of 1, else return False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliBandit(MultiArmedBandit):\n",
    "    def __init__(self, num_bandits):\n",
    "        super(BernoulliBandit, self).__init__(num_bandits)\n",
    "        \n",
    "    def chooseBandit(self, bandit_num):\n",
    "        \"\"\"\n",
    "        Chooses the indicated bandit and returns True if we get a reward, False otherwise.\n",
    "        \"\"\"\n",
    "        # Index out of range\n",
    "        assert bandit_num >= 0 and bandit_num < self.num_bandits, \\\n",
    "            \"Index Out of Range: {}. There are {} bandits.\".format(bandit_num, self.num_bandits)\n",
    "            \n",
    "        # Sample from Bernoulli distribution with probability of success equal to the bandit mean\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE THIS TO EXPERIMENT\n",
    "num_bandits = 10\n",
    "num_iterations = 10000\n",
    "\n",
    "TS_bandits = BernoulliBandit(num_bandits)\n",
    "UCB_bandits = copy.deepcopy(TS_bandits)    # To ensure that they are instantiated with the same bandit means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Thompson Sampling and UCB Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_sample_means_TS(stats):\n",
    "    \"\"\"\n",
    "    USED FOR THOMPSON SAMPLING.\n",
    "    \n",
    "    Samples the probability of each bandit yielding a reward given the number of successes and failures so far.\n",
    "    This is computed using the Beta distribution.\n",
    "    \n",
    "    Parameters:\n",
    "    stats = [{'num_wins': __, 'num_losses': __}, ...]\n",
    "    len(stats) should be equal to the number of bandits\n",
    "    \n",
    "    Return:\n",
    "    reward_probs: array of reward probability for each bandit\n",
    "    \"\"\"\n",
    "    reward_probs = np.zeros(len(stats))\n",
    "    raise NotImplementedError\n",
    "\n",
    "def compute_heuristic_UCB(stats, curr_iteration=None):\n",
    "    \"\"\"\n",
    "    USED FOR UCB.\n",
    "    \n",
    "    Computes the heuristic (average reward for bandit i) + sqrt(2*ln(n)/(# times bandit i is chosen))\n",
    "    where n is the number of actions taken so far.\n",
    "    \n",
    "    Parameters:\n",
    "    stats = [{'num_wins': __, 'num_losses': __}, ...]\n",
    "    curr_iteration = number of iterations passed/actions taken so far\n",
    "    len(stats) should be equal to the number of bandits\n",
    "    \n",
    "    Return:\n",
    "    heuristics: array of UCB heuristic values\n",
    "    \"\"\"\n",
    "    heuristics = np.zeros(len(stats))\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runSamplingAlgo(bandits, sampling_algo):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    bandits (MultiArmedBandit): instance of MultiArmedBandit class\n",
    "    sampling_algo (str): sampling algorithm to use\n",
    "    \n",
    "    Return:\n",
    "    round_stats: array of bandit chosen and regret at each iteration\n",
    "    \"\"\"\n",
    "    # bandit_stats contains number of wins and losses for each bandit\n",
    "    bandit_stats = np.zeros(num_bandits, dtype=[('num_wins', np.int), ('num_losses', np.int)])\n",
    "    # round_stats contains bandit chosen and regret at each iteration\n",
    "    round_stats = np.zeros(num_iterations, dtype=[('bandit', np.int), ('regret', np.float64)])\n",
    "    \n",
    "    for i in range(1, num_iterations):\n",
    "        # Deep copy statistics from the previous iteration\n",
    "        round_stats[i] = round_stats[i - 1].copy()\n",
    "        # TODO: Compute the sampling heuristic for each bandit based on given algorithm\n",
    "        \n",
    "        # TODO: Choose the bandit with the maximum heuristic value\n",
    "        \n",
    "        # TODO: Update the round_stats and bandit_stats given selected bandit\n",
    "        \n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run both Thompson Sampling and UCB algorithm to get round_stats for each algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TS_round_stats = # Fill in\n",
    "UCB_round_stats = # Fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot round stats to compare the two algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the chosen bandit at each iteration\n",
    "fig, ax = plt.subplots(1, 2, figsize=(13,5))\n",
    "ax[0].scatter(range(num_iterations), TS_round_stats['bandit'], marker='x')\n",
    "ax[0].set_yticks(range(num_bandits))\n",
    "ax[0].set_xlabel(\"Iteration Number\")\n",
    "ax[0].set_ylabel(\"Selected Bandit\")\n",
    "ax[0].set_title(\"Bandit chosen at each iteration (Thompson Sampling)\")\n",
    "\n",
    "ax[1].scatter(range(num_iterations), UCB_round_stats['bandit'], marker='x')\n",
    "ax[1].set_yticks(range(num_bandits))\n",
    "ax[1].set_xlabel(\"Iteration Number\")\n",
    "ax[1].set_ylabel(\"Selected Bandit\")\n",
    "ax[1].set_title(\"Bandit chosen at each iteration (UCB)\")\n",
    "\n",
    "max_mean_index = np.argmax(TS_bandits.bandits)\n",
    "for i in range(num_bandits):\n",
    "    if i == max_mean_index:\n",
    "        ax[1].text(x=num_iterations+700, y=i-0.2, s=\"$\\mu_{}$ = {}\".format(i, TS_bandits.bandits[i]), color=\"red\")\n",
    "    else:\n",
    "        ax[1].text(x=num_iterations+700, y=i-0.2, s=\"$\\mu_{}$ = {}\".format(i, TS_bandits.bandits[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accumulated regret through the experiment\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20,5))\n",
    "ax[0].plot(range(num_iterations), TS_round_stats['regret'])\n",
    "ax[0].set_xlabel(\"Iteration Number\")\n",
    "ax[0].set_ylabel(\"Cumulative Regret\")\n",
    "ax[0].set_title(\"Total Regret (Thompson Sampling)\")\n",
    "\n",
    "ax[1].plot(range(num_iterations), UCB_round_stats['regret'])\n",
    "ax[1].set_xlabel(\"Iteration Number\")\n",
    "ax[1].set_ylabel(\"Cumulative Regret\")\n",
    "ax[1].set_title(\"Total Regret (UCB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Thompson Sampling for General Stochastic Bandits <a name=\"section2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we'll extend the Thompson Sampling algorithm to general stochastic bandit case, where:\n",
    "* Rewards $r_i \\in [0,1]$ drawn from an arbitrary unknown distribution\n",
    "* Each arm $i$ has mean $\\mu_i$\n",
    "\n",
    "However, we will adapt the algorithm in a way that allows us to reuse the analysis of the Bernoulli Bandit. We will again make use of the Beta distribution as a convenient prior on the bandit arms.\n",
    "\n",
    "The idea is that after observing the reward $\\tilde{r}_t \\in [0,1]$ at time $t$, we will perform a Bernoulli trial with probability of success $p = \\tilde{r}_t$. Let the outcome of this Bernoulli trial be $r_t \\in \\{0,1\\}$. Again, let $S_i(t)$ and $F_i(t)$ be the number of successes and failures up to time $t$, respectively, as before. \n",
    "\n",
    "Notice that the probability of observing a success in the Bernoulli trial after playing an arm $i$ in the new algorithm is the mean of that arm $\\mu_i$. If we let $f_i$ denote the pdf of the unknown reward distribution for arm $i$. So, when playing arm $i$:\n",
    "$$\\Pr(r_t=1) = \\int_{0}^{1} \\tilde{r}f_i(\\tilde{r})d\\tilde{r} = \\mu_i$$\n",
    "\n",
    "---\n",
    "#### Algorithm 2: Thompson Sampling for General Stochastic Bandits\n",
    "For each arm $i=1,...,N$, set $S_i(1)=0$, $F_i(1)=0$\n",
    "\n",
    "For each  $t=1,2,...$ do\n",
    "\n",
    "* For each arm $i=1,...,N$, sample $\\theta_i(t)$ from the Beta($S_i(t)+1$, $F_i(t)+1$) distribution\n",
    "* Play arm $i(t) := argmax_i$ $\\theta_i(t)$ and observe reward $\\tilde{r}_t$\n",
    "* Perform a Bernoulli trial with success probability $\\tilde{r}_t$ and observe output $r_t$\n",
    "* If $r_t=1$, then $S_i(t) = S_i(t)+1$, else $F_i(t) = F_i(t)+1$\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement chooseBandit function. Returns True if bandit yields reward 1, else return False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniformBandit(MultiArmedBandit):\n",
    "    def __init__(self, num_bandits):\n",
    "        super(UniformBandit, self).__init__(num_bandits)\n",
    "        \n",
    "    def chooseBandit(self, bandit_num):\n",
    "        \"\"\"\n",
    "        Chooses the indicated bandit and returns True if we get a reward, False otherwise.\n",
    "        \"\"\"\n",
    "        # Index out of range\n",
    "        assert bandit_num >= 0 and bandit_num < self.num_bandits, \\\n",
    "            \"Index Out of Range: {}. There are {} bandits.\".format(bandit_num, self.num_bandits)\n",
    "            \n",
    "        # Sample from uniform distribution with probability of success equal to the bandit mean\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run sampling algorithm and plot as done before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE THIS TO EXPERIMENT\n",
    "num_bandits = 10\n",
    "num_iterations = 10000\n",
    "\n",
    "TS_bandits = UniformBandit(num_bandits)\n",
    "UCB_bandits = copy.deepcopy(TS_bandits)    # To ensure that they are instantiated with the same bandit means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TS_round_stats = # Fill in\n",
    "UCB_round_stats = # Fill in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the chosen bandit at each iteration\n",
    "fig, ax = plt.subplots(1, 2, figsize=(13,5))\n",
    "ax[0].scatter(range(num_iterations), TS_round_stats['bandit'], marker='x')\n",
    "ax[0].set_yticks(range(num_bandits))\n",
    "ax[0].set_xlabel(\"Iteration Number\")\n",
    "ax[0].set_ylabel(\"Selected Bandit\")\n",
    "ax[0].set_title(\"Bandit chosen at each iteration (Thompson Sampling)\")\n",
    "\n",
    "ax[1].scatter(range(num_iterations), UCB_round_stats['bandit'], marker='x')\n",
    "ax[1].set_yticks(range(num_bandits))\n",
    "ax[1].set_xlabel(\"Iteration Number\")\n",
    "ax[1].set_ylabel(\"Selected Bandit\")\n",
    "ax[1].set_title(\"Bandit chosen at each iteration (UCB)\")\n",
    "\n",
    "max_mean_index = np.argmax(TS_bandits.bandits)\n",
    "for i in range(num_bandits):\n",
    "    if i == max_mean_index:\n",
    "        ax[1].text(x=num_iterations+700, y=i-0.2, s=\"$\\mu_{}$ = {}\".format(i, TS_bandits.bandits[i]), color=\"red\")\n",
    "    else:\n",
    "        ax[1].text(x=num_iterations+700, y=i-0.2, s=\"$\\mu_{}$ = {}\".format(i, TS_bandits.bandits[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accumulated regret through the experiment\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20,5))\n",
    "ax[0].plot(range(num_iterations), TS_round_stats['regret'])\n",
    "ax[0].set_xlabel(\"Iteration Number\")\n",
    "ax[0].set_ylabel(\"Cumulative Regret\")\n",
    "ax[0].set_title(\"Total Regret (Thompson Sampling)\")\n",
    "\n",
    "ax[1].plot(range(num_iterations), UCB_round_stats['regret'])\n",
    "ax[1].set_xlabel(\"Iteration Number\")\n",
    "ax[1].set_ylabel(\"Cumulative Regret\")\n",
    "ax[1].set_title(\"Total Regret (UCB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
