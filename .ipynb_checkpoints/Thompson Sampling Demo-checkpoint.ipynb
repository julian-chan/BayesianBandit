{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Bandits: An Application of Thompson Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements the Thompson Sampling algorithm described in [Analysis of Thompson Sampling for the Multi-Armed Bandit Problem](http://proceedings.mlr.press/v23/agrawal12/agrawal12.pdf) by Shipra Agrawal and Navin Goyal and applies to the Bayesian Bandit problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "[Section 1: Thompson Sampling for Bernoulli Bandits](#section1)\n",
    "\n",
    "[Section 2: Thompson Sampling for General Stochastic Bandits](#section2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Thompson Sampling for Bernoulli Bandits <a name=\"section1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first examine the Bernoulli Bandit case where:\n",
    "* Rewards $r_t \\in \\{0, 1\\}$\n",
    "* For arm $i$, the probability of success ($r_{t,i} = 1$) is its mean, $\\mu_i$\n",
    "\n",
    "We maintain a Bayesian prior on the arm means $\\mu_i$. For Bernoulli rewards (either 0 or 1), it turns out that a Beta distribution is a convenient choice of priors. This is because if the prior is a Beta($\\alpha$, $\\beta$) distribution, then updating the posterior distributions become much simpler. After observing a Bernoulli trial, the posterior distribution is:\n",
    "* Beta($\\alpha+1$, $\\beta$) if the trial succeeded (reward = 1)\n",
    "* Beta($\\alpha$, $\\beta+1$) if the trial failed (reward = 0)\n",
    "\n",
    "The Thompson Sampling algorithm initializes a uniform prior on all arms, meaning that arm $i$ has a prior Beta(1,1) on $\\mu_i$ because Beta(1,1) is a uniform distribution on (0,1).\n",
    "\n",
    "Define:\n",
    "* $S_i(t)$ = number of successes (reward = 1) for arm $i$ up to time $t$\n",
    "* $F_i(t)$ = number of failures (reward = 0) for arm $i$ up to time $t$\n",
    "\n",
    "The algorithm will then update the distribution on $\\mu_i$ as Beta($S_i(t)+1$, $F_i(t)+1$), sample from these posterior distributions, and play an arm according to the probability of its mean being the largest.\n",
    "\n",
    "---\n",
    "#### Algorithm 1: Thompson Sampling for Bernoulli Bandits\n",
    "For each arm $i=1,...,N$, set $S_i=0$, $F_i=0$\n",
    "\n",
    "For each  $t=1,2,...$ do\n",
    "\n",
    "* For each arm $i=1,...,N$, sample $\\theta_i(t)$ from the Beta($S_i+1$, $F_i+1$) distribution\n",
    "* Play arm $i(t) := argmax_i$ $\\theta_i(t)$ and observe reward $r_t$\n",
    "* If $r_t=1$, then $S_i(t) = S_i(t)+1$, else $F_i(t) = F_i(t)+1$\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Thompson Sampling for General Stochastic Bandits <a name=\"section2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
